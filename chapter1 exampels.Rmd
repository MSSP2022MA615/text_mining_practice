---
title: "text mining ch1"
author: "Hao He"
date: "2022-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidytext)
library(tidyverse)
library(dplyr)
library(gutenbergr)
library(janeaustenr)
library(stringr)
```


# Examples from Chapter 1.

Some lovely text by Emily Dickinson in her time:
```{r}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")

text 
#> [1] "Because I could not stop for Death -"  
#> [2] "He kindly stopped for me -"            
#> [3] "The Carriage held but just Ourselves -"
#> [4] "and Immortality"
```

To turn this character vector here into a tidy text dataset, first need to put it into a data frame.

```{r}
library(dplyr)
text_df <- tibble(line = 1:4, text = text)

text_df
#> # A tibble: 4 × 2
#>    line text                                  
#>   <int> <chr>                                 
#> 1     1 Because I could not stop for Death -  
#> 2     2 He kindly stopped for me -            
#> 3     3 The Carriage held but just Ourselves -
#> 4     4 and Immortality
```

since each row is made up of multiple combined words. We need to convert this so that it has one-token-per-document-per-row. Token here is a word. Tokenization refers to break the text into individual tokens. 

we need to both do tokenization and transform it to a tidy data structure. To do this, we use tidytext’s unnest_tokens() function.

## The unnest_tokens function
```{r}
library(tidytext)

text_df %>%
  unnest_tokens(word, text) #word refers to the column name that will be created at your choice to store a token, text refers to the column name in the tibble we created before

# Punctuation has been stripped.
# By default, unnest_tokens() converts the tokens to lowercase, which makes them easier to compare or combine with other datasets. (Use the to_lower = FALSE argument to turn off this behavior).
```

## Tidying Jane Austen book

```{r}
original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>%
  ungroup()
# mutate to annotate a chapter (using a regex) to find where all the chapters are.
original_books
```

restructure it in the one-token-per-row format using *unnest_tokens()*, then we have a tidy dataset.
```{r}
tidy_books <- original_books %>%
  unnest_tokens(word, text)
```

Next, in analysis, remove stop words (typically extremely common words in English) using *anti_join()*.
```{r}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words) # returns what is not a match in the set of stop_words 
```

count() to find the most mommon words in all the books
```{r}
tidy_books %>%
  count(word, sort = TRUE) 
```

we have a tidy dataset and now we can pipe this directly to the ggplot2 to create visualization. For example,
visualization of words that Jane Austin used at least 600 times:
```{r}
library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```
## Tidying the gutenbergr package
### word frequencies
#### Get more text from the Gutenberg Project: H.G. Wellsdownloading books for textmining
```{r}

texts <- gutenberg_download(c(768, 1260), meta_fields = "title", 
                            mirror = "http://mirrors.xmission.com/gutenberg/")
```


```{r}
library(gutenbergr)

hgwells <- gutenberg_download(c(35, 36, 5230, 159), mirror = "http://mirrors.xmission.com/gutenberg/")

tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# most common words used by H.G Wells
tidy_hgwells %>%
  count(word, sort = TRUE)

# plot the frequencies
tidy_hgwells %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## Get more text from Brontë sisters
```{r}
 # We will again use the Project Gutenberg ID numbers for each novel and access the texts using gutenberg_download().
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767),mirror = "http://mirrors.xmission.com/gutenberg/")


tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

# count common words
tidy_bronte %>%
  count(word, sort = TRUE)

# plot the frequencies
tidy_bronte %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

## binding the three data frames together, reshape using pivot_wider and pivot_longer.
```{r}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = author, values_from = proportion) %>%
  pivot_longer(`Brontë Sisters`:`H.G. Wells`,
               names_to = "author", values_to = "proportion")

frequency
#> # A tibble: 57,820 × 4
#>    word    `Jane Austen` author          proportion
#>    <chr>           <dbl> <chr>                <dbl>
#>  1 a          0.00000919 Brontë Sisters  0.0000319 
#>  2 a          0.00000919 H.G. Wells      0.0000150 
#>  3 a'most    NA          Brontë Sisters  0.0000159 
#>  4 a'most    NA          H.G. Wells     NA         
#>  5 aback     NA          Brontë Sisters  0.00000398
#>  6 aback     NA          H.G. Wells      0.0000150 
#>  7 abaht     NA          Brontë Sisters  0.00000398
#>  8 abaht     NA          H.G. Wells     NA         
#>  9 abandon   NA          Brontë Sisters  0.0000319 
#> 10 abandon   NA          H.G. Wells      0.0000150 
#> # … with 57,810 more rows
```

## Now let's plotting and comparing the three sets of novels.

Words that are *close to the line* in these plots have similar frequencies in both sets of texts.

Words that are *far from the line* are words that are found more in one set of texts than another.
```{r}
library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `Jane Austen`, 
                      color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)

```
## Use correlation test to quantify the similarity and difference between these sets of word frequencies.
```{r}
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)
#> 
#>  Pearson's product-moment correlation
#> 
#> data:  proportion and Jane Austen
#> t = 119.64, df = 10404, p-value < 2.2e-16
#> alternative hypothesis: true correlation is not equal to 0
#> 95 percent confidence interval:
#>  0.7527837 0.7689611
#> sample estimates:
#>       cor 
#> 0.7609907
cor.test(data = frequency[frequency$author == "H.G. Wells",], 
         ~ proportion + `Jane Austen`)
#> 
#>  Pearson's product-moment correlation
#> 
#> data:  proportion and Jane Austen
#> t = 36.441, df = 6053, p-value < 2.2e-16
#> alternative hypothesis: true correlation is not equal to 0
#> 95 percent confidence interval:
#>  0.4032820 0.4446006
#> sample estimates:
#>      cor 
#> 0.424162
```

